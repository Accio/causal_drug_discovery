---
title: "Causal inference workshop by Richard McElreath"
author: "Jitao david Zhang"
date: "29/11/2021"
output: html_document
---

This document walks through the models introduced in the causal inference workshop given by Richard McElreath.

* [The workshop, Science Before Statistics: Causal Inference](https://www.youtube.com/watch?v=KNPYUVmY3NM&t=2084s)
* [The original R script posted on GitHub](https://github.com/rmcelreath/causal_salad_2021/blob/main/1_causal_salad.r)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.height = 4, 
                      fig.width = 4)
library(ribiosUtils)
library(ribiosIO)
library(rethinking)
library(dagitty)
library(ggplot2)
library(ggdag)
library(cmdstanr) ## installation instructions available at https://mc-stan.org/cmdstanr/articles/cmdstanr.html
library(ggpmisc)
library(ggthemes)
library(gridExtra)
ggplot2::theme_set(theme_minimal(base_size=14))
```

# Basics

## The simplest relation: two variables, one causing the other

```{r}
set.seed(1887)
N <- 50
x <- rnorm(N, mean=5)
y <- rnorm(N, mean=x*2)
xydata <- data.frame(x=x, y=y)
xylm <- lm(y~x, data=xydata)
my.formula <- y ~ x
yxPlot <- ggplot(xydata, aes(x=x, y=y)) + geom_point() +
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label..,  ..rr.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  ggtitle("True effect: 2.0")
print(yxPlot)
```

```{r}
View(xydata)
```

```{r fig.height=3, fig.width=5}
summary(lm(y ~ x, data=xydata))
```

```{r xCy, fig.height=3, fig.width=3}
set.seed(1887)
model0 <- dagify(y ~ x,
                 exposure = "x",
                 outcome = "y")
model0 <- tidy_dagitty(model0,
                       layout="linear")
xCausesY <- ggplot(model0, aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_fan() +
      geom_dag_text(size=12, vjust=0.3) +
      theme_dag()
print(xCausesY)
```

```{r yCx, fig.height=3, fig.width=3}
set.seed(1887)
model0r <- dagify(x ~ y,
                 exposure = "y",
                 outcome = "x")
model0r <- tidy_dagitty(model0r,
                       layout="linear")
yCausesX <- ggplot(model0r, aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_fan() +
      geom_dag_text(size=12, vjust=0.3) +
      theme_dag()
print(yCausesX)
```

What happens if we switch target variable with the random variable? Well, we get an equally well fit.

```{r fig.width=6.5, fig.height=3.25}
xyPlot <- ggplot(xydata, aes(x=y, y=x)) + geom_point() +
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
                        eq.x.rhs = "y",
               eq.with.lhs = "italic(hat(x))~`=`~",
               aes(label = paste(..eq.label..,  ..rr.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  ggtitle("The reverse fit")
grid.arrange(grobs=list(yxPlot, xyPlot), nrow=1, ncol=2)
```

```{r learningCausal}
set.seed(1887)
xknock <- rnorm(3, mean=1, sd=0.5)
xCauseYpred <- data.frame(x=xknock, 
                          y=predict(lm(y~x, data=xydata), newdata=list(x=xknock)))
yCauseXpred <- data.frame(x=xknock, y=rnorm(3, mean=median(xydata$y)))

ggplot(xydata, aes(x=x, y=y)) + geom_point() +
  ggpmisc::stat_poly_eq(formula = my.formula,
                        eq.x.rhs = "y",
               eq.with.lhs = "italic(hat(x))~`=`~",
               aes(label = paste(..eq.label..,  ..rr.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  xlim(0, 8) + ylim(0, 16) +
  geom_vline(xintercept=1, lty=2) +
  geom_point(data=xCauseYpred, col="red", shape=8, cex=4) +
  geom_point(data=yCauseXpred, col="navyblue", shape=4, cex=4)
```

## A binary case

```{r}
set.seed(1887)
N <- 50
x <- rbern(N, prob=0.5)
y <- rnorm(N, mean=x*2)
xybid <- data.frame(x=factor(x), y=y)
ggplot(xybid, aes(x=x, y=y)) + geom_boxplot() + geom_point() + 
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  ggtitle("True effect: 2.0")
```

## A chain

```{r, fig.height=3, fig.width=9}
set.seed(1888)
N <- 50
x <- rnorm(N, mean=5)
z <- ifelse(x<5, -1, 1)
y <- rnorm(N, mean=z*1.5)
xzydata <- data.frame(x=x, z=z, y=y)
xz.formula <- z ~ x
zy.formula <- y ~ z
xzPlot <- ggplot(xzydata, aes(x=x, y=z)) + geom_point() +
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(z))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE)
zyPlot <- ggplot(xzydata, aes(x=z, y=y)) + geom_point() +
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
                        eq.x.rhs = "z",
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE)
xyPlot <- ggplot(xzydata, aes(x=x, y=y)) + geom_point() +
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE)
grid.arrange(grobs=list(xzPlot, zyPlot, xyPlot), nrow=1, ncol=3)
```

```{r chainCondition}
chainYvX <- lm(y~x, data=xzydata)
chainYvXcZ <- lm(y~x+z, data=xzydata)
with(xzydata,
     plot(coeftab(lm(y ~ x),
                  lm(y ~ x + z)),
              pars="x"))
```

```{r chainModel, fig.height=3, fig.width=3}
set.seed(1888)
chainModel <- dagify(z ~ x,
                 y ~ z,
                 exposure = "x",
                 outcome="y") %>%
  tidy_dagitty(layout="linear")
ggplot(chainModel, aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_link() +
      geom_dag_text(size=12, vjust=0.3) +
      theme_dag()
```
## A fork

```{r modelFork, fig.height=3, fig.width=3}
set.seed(1888)
modelFork <- dagify(y ~ z,
                    x ~ z)%>%
  tidy_dagitty(seed=2, layout="tree")
ggplot(modelFork, aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_link() +
      geom_dag_text(size=12, vjust=0.3) +
      theme_dag()
```

```{r forkFit, fig.height=3, fig.width=7}
set.seed(1888)
N <- 50
z <- rbern(50, 0.5)
x <- rnorm(N, mean=z)
y <- rnorm(N, mean=z)
xzydata <- data.frame(x=x, z=factor(z), y=y)
xz.formula <- z ~ x
zy.formula <- y ~ z
xyPlot <- ggplot(xzydata, aes(x=x, y=y)) + geom_point() +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  stat_quant_band(formula = y~x)
xyCondZPlot <- ggplot(xzydata, aes(x=x, y=y, col=z)) + geom_point() +
  geom_point() +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  stat_quant_band(formula = y~x)

grid.arrange(grobs=list(xyPlot, xyCondZPlot), nrow=1, ncol=2)
```
```{r fig.height=3, fig.width=4.5}
xzPlot <- ggplot(xzydata, aes(x=z, y=x)) + geom_boxplot() + geom_point() + 
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(x))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  ggtitle("True effect: 1.0")
yzPlot <- ggplot(xzydata, aes(x=z, y=y)) + geom_boxplot() + geom_point() + 
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  ggtitle("True effect: 1.0")
grid.arrange(grobs=list(xzPlot, yzPlot), nrow=1, ncol=2)
```

```{r}
View(xzydata)
```

## A collider

```{r modelCollide, fig.height=3, fig.width=3}
set.seed(1888)
modelCollider <- dagify(z ~ x,
                 z ~ y) %>%
  tidy_dagitty(layout="sugiyama")
ggplot(modelCollider, aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_link() +
      geom_dag_text(size=12, vjust=0.3) +
      theme_dag()
```

```{r colliderFit, fig.height=3, fig.width=7}
set.seed(1888)
N <- 50
x <- rnorm(N)
y <- rnorm(N)
z <- ifelse(x+y>0, 1, -1)
xzydata <- data.frame(x=x, z=factor(z), y=y)
xz.formula <- z ~ x
zy.formula <- y ~ z
xyPlot <- ggplot(xzydata, aes(x=x, y=y)) + geom_point() +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  stat_quant_band(formula = y~x)
xyCondZPlot <- ggplot(xzydata, aes(x=x, y=y, col=z)) + geom_point() +
  geom_point(data=filter(xzydata, z==-1), mapping=aes(x=x, y=y), col="mistyrose",
             inherit.aes = FALSE) +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  stat_quant_band(formula = y~x)
grid.arrange(grobs=list(xyPlot, xyCondZPlot), nrow=1, ncol=2)
```

# The two moms example

```{r, fig.height=3, fig.width=3}
tm <- dagitty("dag {
              B1 -> M ;
              M -> D;
              B2 -> D }")
plot(tm)
```

```{r twoMomsWoConfound}
set.seed(1887)
N <- 200 # number of pairs

B1 <- rbinom(N, size=1, prob=0.5) # 50% first borns
M <- rnorm(N, 2*B1)
B2 <- rbinom(N, size=1, prob=0.5)
D <- rnorm(N, 2*B2 + 0*M) # change the 0 to another number to turn on the causal influence of mom

summary(lm(D ~ M))
summary(lm(D ~ M + B1))
summary(lm(D ~ M + B2))

plot(coeftab(lm(D ~ M),
             lm(D ~ M + B1),
             lm(D ~ M + B2)),
              pars="M")
```
Given the causal model, adding B1 to the model leads to worse precision, while adding B2 has better precision.

Now with confound.

```{r, fig.height=3, fig.width=3}
tmc <- dagitty("dag {
              B1 -> M ;
              M -> D;
              B2 -> D;
              U -> D;
              U -> M}")
plot(tmc)
```

```{r twoMomWithConfound}
set.seed(1887)
N <- 200
U <- rnorm(N)
B1 <- rbinom(N, size=1, prob=0.5)
M <- rnorm(N, 2*B1 + U)
B2 <- rbinom(N, size=1, prob=0.5)
D <- rnorm(N, 2*B2 + U + 0*M )

# fit the regression models
summary(lm(D ~ M))
summary(lm(D ~ M + B1))
summary(lm(D ~ M + B2))
summary(lm(D ~ M + B1 + B2))

plot(coeftab(lm(D ~ M),
             lm(D ~ M + B1),
             lm(D ~ M + B2),
             lm(D ~ M + B1 + B2)),
              pars="M")
```

Now a model including B1 is seriously wrong.

We look at the Akaike Infomation Criterion (AIC), a commonly used estimator of prediction error and a quality-control tool of statistical models for a  given set of data, of the models. Having B1 or not has no significant effect.

```{r aic}
AIC(lm(D ~ M))
AIC(lm(D ~ M + B1))
```

## The Bayesian inference method

Best scenario: the confound is observed. In reality this is probably difficult.

```{r}
precis(lm(D ~ M + B2 + U))
```

What can Bayesian inference bring us if the confounding factor is not observable? Below we build a STAN model specifying the DAG above, in particular the unmeasured confound U, as well as the priors. Then the model runs to derive posterior distribution of the parameters. FBI=full bayesian inference (replacing Richar'ds Full Luxury Bayesian Inference, flbi).

```{r stan}
dat <- list(N=N,M=M,D=D,B1=B1,B2=B2)
set.seed(1887)
fbi <- ulam(
    alist(
        # mom model
            M ~ normal( mu , sigma ),
            mu <- a1 + b*B1 + k*U[i],
        # daughter model
            D ~ normal( nu , tau ),
            nu <- a2 + b*B2 + m*M + k*U[i],
        # B1 and B2
            B1 ~ bernoulli(p),
            B2 ~ bernoulli(p),
        # unmeasured confound
            vector[N]:U ~ normal(0,1),
        # priors
            c(a1,a2,b,m) ~ normal( 0 , 0.5 ),
            c(k,sigma,tau) ~ exponential( 1 ),
            p ~ beta(2,2)
    ), data=dat , chains=4 , cores=4 , iter=2000 , cmdstan=TRUE )

precis(fbi)
```

Comparing the results of Bayesian inference and linear regressions above.

```{r}
m <- M
plot( coeftab( lm( D ~ m ) , lm( D ~ m + B1 ) ,  lm( D ~ m + B2 ) , fbi ) , pars="m" )
```

A strength of a Bayesian inference model is that it is a generative model, which means that we can extract or draw samples from the fit models. We can compare simulated with estimated confounding factors.

```{r}
post <- extract.samples(fbi)
Uest <- apply(post$U,2,mean) 
{
  blank()
  plot(U, Uest, xlab="U (simulated)",ylab="U (estimated)", col=2 , lwd=2 )
  abline(a=0,b=1,lty=2)
}
```

Alternative to setting up an explicit term for the unobserved confounding factor, we can also assume that mom and daughter are multi-normally distributed.

```{r}
# version that marginalizes out the missing data
fbi_plus <- ulam(
    alist(
        c(M,D) ~ multi_normal( c(mu,nu) , Rho , Sigma ),
        mu <- a1 + b*B1,
        nu <- a2 + b*B2 + m*M,
        c(a1,a2,b,m) ~ normal( 0 , 0.5 ),
        Rho ~ lkj_corr( 2 ),
        Sigma ~ exponential( 1 )
    ), data=dat , chains=4 , cores=4 , cmdstan=TRUE )

precis(fbi_plus,3)
```

Below is a more exotic example, where there is no instrument variable (B1 -> D), but two measures of U (V and W)
```{r}
set.seed(1887)
N <- 200 # number of pairs
U <- rnorm(N,0,1) # simulate confound
V <- rnorm(N,U,1)
W <- rnorm(N,U,1)
# birth order and family sizes
B1 <- rbinom(N,size=1,prob=0.5) # 50% first borns
M <- rnorm( N , 2*B1 + U )
B2 <- rbinom(N,size=1,prob=0.5)
D <- rnorm( N , 2*B2 + 0.5*B1 + U + 0*M )

# confounded regression
precis( lm( D ~ M + B1 + B2 + V + W ) )

# full-luxury bayesian inference
dat2 <- list(N=N,M=M,D=D,B1=B1,B2=B2,V=V,W=W)
fbi2 <- ulam(
    alist(
        M ~ normal( muM , sigmaM ),
        muM <- a1 + b*B1 + k*U[i],
        D ~ normal( muD , sigmaD ),
        muD <- a2 + b*B2 + d*B1 + m*M + k*U[i],
        W ~ normal( muW , sigmaW ),
        muW <- a3 + w*U[i],
        V ~ normal( muV , sigmaV ),
        muV <- a4 + v*U[i],
        vector[N]:U ~ normal(0,1),
        c(a1,a2,a3,a4,b,d,m) ~ normal( 0 , 0.5 ),
        c(k,w,v) ~ exponential( 1 ),
        c(sigmaM,sigmaD,sigmaW,sigmaV) ~ exponential( 1 )
    ), data=dat2 , chains=4 , cores=4 , iter=2000 , cmdstan=TRUE )

precis(fbi2)
```

# The peer bias example

To finish.