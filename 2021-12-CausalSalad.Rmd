---
title: "Causal inference workshop by Richard McElreath"
author: "Jitao david Zhang"
date: "29/11/2021"
output: html_document
---

This document walks through the models introduced in the causal inference workshop given by Richard McElreath.

* [The workshop, Science Before Statistics: Causal Inference](https://www.youtube.com/watch?v=KNPYUVmY3NM&t=2084s)
* [The original R script posted on GitHub](https://github.com/rmcelreath/causal_salad_2021/blob/main/1_causal_salad.r)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.height = 4, 
                      fig.width = 4)
library(ribiosUtils)
library(ribiosIO)
library(rethinking)
library(dagitty)
library(ggplot2)
library(ggdag)
library(cmdstanr) ## installation instructions available at https://mc-stan.org/cmdstanr/articles/cmdstanr.html
library(ggpmisc)
library(ggthemes)
library(gridExtra)
ggplot2::theme_set(theme_minimal(base_size=14))
```

# Basics

## The simplest relation: two variables, one causing the other

```{r}
set.seed(1887)
N <- 50
x <- rnorm(N, mean=5)
y <- rnorm(N, mean=x*2)
xydata <- data.frame(x=x, y=y)
xylm <- lm(y~x, data=xydata)
my.formula <- y ~ x
yxPlot <- ggplot(xydata, aes(x=x, y=y)) + geom_point() +
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label..,  ..rr.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  ggtitle("True effect: 2.0")
print(yxPlot)
```

```{r}
View(xydata)
```

```{r fig.height=3, fig.width=5}
summary(lm(y ~ x, data=xydata))
```

```{r xCy, fig.height=3, fig.width=3}
set.seed(1887)
model0 <- dagify(y ~ x,
                 exposure = "x",
                 outcome = "y")
model0 <- tidy_dagitty(model0,
                       layout="linear")
xCausesY <- ggplot(model0, aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_fan() +
      geom_dag_text(size=12, vjust=0.3) +
      theme_dag()
print(xCausesY)
```

```{r yCx, fig.height=3, fig.width=3}
set.seed(1887)
model0r <- dagify(x ~ y,
                 exposure = "y",
                 outcome = "x")
model0r <- tidy_dagitty(model0r,
                       layout="linear")
yCausesX <- ggplot(model0r, aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_fan() +
      geom_dag_text(size=12, vjust=0.3) +
      theme_dag()
print(yCausesX)
```

Yet another possibility by the Reichenbach causality theorem is that both X and Y are regulated by a common cause U.

```{r latent}
set.seed(1887)
latmod <- dagify(x ~ u,
                 y ~ u,
                 latent  = "u",
                 coords = list(x = c(x=0, u=1, y=2),
                               y= c(x=0, u=0, y=0))) %>%
  tidy_dagitty(layout="auto")
latmodPlot <- ggplot(latmod %>%
  dplyr::mutate(latent = ifelse(name %in% "u", TRUE, FALSE)),
  aes(x = x, y = y, xend = xend, yend = yend, color=latent)) +
  scale_color_manual(values=c("black", "lightgrey"), guide="none") +
      geom_dag_point() +
      geom_dag_edges_fan() +
      geom_dag_text(size=12, vjust=0.3, color="white") +
      theme_dag()
print(latmodPlot)
```

```{r latmodSim}
set.seed(1887)
N <- 50
u <- rnorm(N, mean=5)
x <- rnorm(N, mean=u)
y <- rnorm(N, mean=u*2)
xyudata <- data.frame(x=x, y=y, u=u)
xyulm <- lm(y~x, data=xyudata)
my.formula <- y ~ x
uyxPlot <- ggplot(xyudata, aes(x=x, y=y)) + geom_point() +
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label..,  ..rr.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  ggtitle("True effect: 0.0")
print(uyxPlot)
```

What happens if we switch target variable with the random variable? Well, we get an equally well fit.

```{r fig.width=9.75, fig.height=3.25}
xyPlot <- ggplot(xydata, aes(x=y, y=x)) + geom_point() +
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
                        eq.x.rhs = "y",
               eq.with.lhs = "italic(hat(x))~`=`~",
               aes(label = paste(..eq.label..,  ..rr.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  ggtitle("The reverse fit")
grid.arrange(grobs=list(yxPlot, xyPlot, uyxPlot), nrow=1, ncol=3)
```

```{r learningCausal}
set.seed(1887)
xknock <- rnorm(3, mean=1, sd=0.5)
xCauseYpred <- data.frame(x=xknock, 
                          y=predict(lm(y~x, data=xydata), newdata=list(x=xknock)))
yCauseXpred <- data.frame(x=xknock, y=rnorm(3, mean=median(xydata$y)))

ggplot(xydata, aes(x=x, y=y)) + geom_point() +
  ggpmisc::stat_poly_eq(formula = my.formula,
                        eq.x.rhs = "y",
               eq.with.lhs = "italic(hat(x))~`=`~",
               aes(label = paste(..eq.label..,  ..rr.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  xlim(0, 8) + ylim(0, 16) +
  geom_vline(xintercept=1, lty=2) +
  geom_point(data=xCauseYpred, col="red", shape=8, cex=4) +
  geom_point(data=yCauseXpred, col="navyblue", shape=4, cex=4)
```

## A binary case

```{r}
set.seed(1887)
N <- 50
x <- rbern(N, prob=0.5)
y <- rnorm(N, mean=x*2)
xybid <- data.frame(x=factor(x), y=y)
ggplot(xybid, aes(x=x, y=y)) + geom_boxplot() + geom_point() + 
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  ggtitle("True effect: 2.0")
```

## A chain

```{r, fig.height=3, fig.width=9}
set.seed(1888)
N <- 50
x <- rnorm(N, mean=5)
z <- ifelse(x<5, -1, 1)
y <- rnorm(N, mean=z*1.5)
xzydata <- data.frame(x=x, z=z, y=y)
xz.formula <- z ~ x
zy.formula <- y ~ z
xzPlot <- ggplot(xzydata, aes(x=x, y=z)) + geom_point() +
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(z))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE)
zyPlot <- ggplot(xzydata, aes(x=z, y=y)) + geom_point() +
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
                        eq.x.rhs = "z",
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE)
xyPlot <- ggplot(xzydata, aes(x=x, y=y)) + geom_point() +
  geom_smooth(method = "lm", se=FALSE, color="black", formula = my.formula) +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE)
grid.arrange(grobs=list(xzPlot, zyPlot, xyPlot), nrow=1, ncol=3)
```

```{r chainCondition}
chainYvX <- lm(y~x, data=xzydata)
chainYvXcZ <- lm(y~x+z, data=xzydata)
with(xzydata,
     plot(coeftab(lm(y ~ x),
                  lm(y ~ x + z)),
              pars="x"))
```

```{r chainModel, fig.height=3, fig.width=3}
set.seed(1888)
chainModel <- dagify(z ~ x,
                 y ~ z,
                 exposure = "x",
                 outcome="y") %>%
  tidy_dagitty(layout="linear")
ggplot(chainModel, aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_link() +
      geom_dag_text(size=12, vjust=0.3) +
      theme_dag()
```
## A fork

```{r modelFork, fig.height=3, fig.width=3}
set.seed(1888)
modelFork <- dagify(y ~ z,
                    x ~ z)%>%
  tidy_dagitty(seed=2, layout="tree")
ggplot(modelFork, aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_link() +
      geom_dag_text(size=12, vjust=0.3) +
      theme_dag()
```
```{r linearFork, fig.height=3, fig.width=3}
set.seed(1887)
lfmod <- dagify(x ~ z,
                y ~ z,
                 coords = list(x = c(x=0, z=1, y=2),
                               y= c(x=0, z=0, y=0))) %>%
  tidy_dagitty(layout="auto")
lfmodPlot <- ggplot(lfmod,
                    aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_fan() +
      geom_dag_text(size=12, vjust=0.3, color="white") +
      theme_dag()
print(lfmodPlot)
```

```{r forkFit, fig.height=3, fig.width=7}
set.seed(1888)
N <- 50
z <- rbern(50, 0.5)
x <- rnorm(N, mean=z)
y <- rnorm(N, mean=z)
xzydata <- data.frame(x=x, z=factor(z), y=y)
xz.formula <- z ~ x
zy.formula <- y ~ z
xyPlot <- ggplot(xzydata, aes(x=x, y=y)) + geom_point() +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  stat_quant_band(formula = y~x)
xyCondZPlot <- ggplot(xzydata, aes(x=x, y=y, col=z)) + geom_point() +
  geom_point() +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  stat_quant_band(formula = y~x)

grid.arrange(grobs=list(xyPlot, xyCondZPlot), nrow=1, ncol=2)
```
```{r fig.height=3, fig.width=4.5}
xzPlot <- ggplot(xzydata, aes(x=z, y=x)) + geom_boxplot() + geom_point() + 
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(x))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  ggtitle("True effect: 1.0")
yzPlot <- ggplot(xzydata, aes(x=z, y=y)) + geom_boxplot() + geom_point() + 
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  ggtitle("True effect: 1.0")
grid.arrange(grobs=list(xzPlot, yzPlot), nrow=1, ncol=2)
```

```{r}
View(xzydata)
```

## A collider

```{r modelCollide, fig.height=3, fig.width=3}
set.seed(1888)
modelCollider <- dagify(z ~ x,
                 z ~ y) %>%
  tidy_dagitty(layout="sugiyama")
ggplot(modelCollider, aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_link() +
      geom_dag_text(size=12, vjust=0.3) +
      theme_dag()
```
```{r linearCollider, fig.height=3, fig.width=3}
set.seed(1887)
lcmod <- dagify(z ~ x,
                z ~ y,
                 coords = list(x = c(x=0, z=1, y=2),
                               y= c(x=0, z=0, y=0))) %>%
  tidy_dagitty(layout="auto")
lcmodPlot <- ggplot(lcmod,
                    aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_fan() +
      geom_dag_text(size=12, vjust=0.3, color="white") +
      theme_dag()
print(lcmodPlot)
```

```{r colliderFit, fig.height=3, fig.width=7}
set.seed(1888)
N <- 50
x <- rnorm(N)
y <- rnorm(N)
z <- ifelse(x+y>0, 1, -1)
xzydata <- data.frame(x=x, z=factor(z), y=y)
xz.formula <- z ~ x
zy.formula <- y ~ z
xyPlot <- ggplot(xzydata, aes(x=x, y=y)) + geom_point() +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  stat_quant_band(formula = y~x)
xyCondZPlot <- ggplot(xzydata, aes(x=x, y=y, col=z)) + geom_point() +
  geom_point(data=filter(xzydata, z==-1), mapping=aes(x=x, y=y), col="mistyrose",
             inherit.aes = FALSE) +
  ggpmisc::stat_poly_eq(formula = my.formula,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "*plain(\",\")~")), 
               parse = TRUE) +
  stat_quant_band(formula = y~x)
grid.arrange(grobs=list(xyPlot, xyCondZPlot), nrow=1, ncol=2)
```
```{r}
View(xzydata)
```


```{r}
xyzPlot <- ggplot(xzydata, aes(x=x+y, y=z)) + geom_point()
print(xyzPlot)
```
## Descendent

```{r descend, fig.height=3, fig.width=3}
set.seed(1887)
ldmod <- dagify(u ~ x,
                u ~ y,
                z ~ u,
                 coords = list(x = c(x=0, u=1, z=1, y=2),
                               y= c(x=0, u=0, z=-1, y=0))) %>%
  tidy_dagitty(layout="auto")
ldmodPlot <- ggplot(ldmod,
                    aes(x = x, y = y, xend = xend, yend = yend)) +
      geom_dag_point() +
      geom_dag_edges_fan() +
  ylim(-2,0.5)+
      geom_dag_text(size=12, vjust=0.3, color="white") +
      theme_dag()
print(ldmodPlot)
```

# The two moms example

```{r, fig.height=3, fig.width=3}
tm <- dagitty("dag {
              B1 -> M ;
              M -> D;
              B2 -> D }")
plot(tm)
```

```{r twoMomsWoConfound}
set.seed(1887)
N <- 200 # number of pairs

B1 <- rbinom(N, size=1, prob=0.5) # 50% first borns
M <- rnorm(N, 2*B1)
B2 <- rbinom(N, size=1, prob=0.5)
D <- rnorm(N, 2*B2 + 0*M) # change the 0 to another number to turn on the causal influence of mom

summary(lm(D ~ M))
summary(lm(D ~ M + B1))
summary(lm(D ~ M + B2))

plot(coeftab(lm(D ~ M),
             lm(D ~ M + B1),
             lm(D ~ M + B2)),
              pars="M")
```
Given the causal model, adding B1 to the model leads to worse precision, while adding B2 has better precision.

Now with confound.

```{r, fig.height=3, fig.width=3}
tmc <- dagitty("dag {
              B1 -> M ;
              M -> D;
              B2 -> D;
              U -> D;
              U -> M}")
plot(tmc)
```

```{r twoMomWithConfound}
set.seed(1887)
N <- 200
U <- rnorm(N)
B1 <- rbinom(N, size=1, prob=0.5)
M <- rnorm(N, 2*B1 + U)
B2 <- rbinom(N, size=1, prob=0.5)
D <- rnorm(N, 2*B2 + U + 0*M )

# fit the regression models
summary(lm(D ~ M))
summary(lm(D ~ M + B1))
summary(lm(D ~ M + B2))
summary(lm(D ~ M + B1 + B2))

plot(coeftab(lm(D ~ M),
             lm(D ~ M + B1),
             lm(D ~ M + B2),
             lm(D ~ M + B1 + B2)),
              pars="M")
```

Now a model including B1 is seriously wrong.

We look at the Akaike Infomation Criterion (AIC), a commonly used estimator of prediction error and a quality-control tool of statistical models for a  given set of data, of the models. Having B1 or not has no significant effect.

```{r aic}
AIC(lm(D ~ M))
AIC(lm(D ~ M + B1))
```

## The Bayesian inference method

Best scenario: the confound is observed. In reality this is probably difficult.

```{r}
precis(lm(D ~ M + B2 + U))
```

What can Bayesian inference bring us if the confounding factor is not observable? Below we build a STAN model specifying the DAG above, in particular the unmeasured confound U, as well as the priors. Then the model runs to derive posterior distribution of the parameters. FBI=full bayesian inference (replacing Richar'ds Full Luxury Bayesian Inference, flbi).

```{r stan}
dat <- list(N=N,M=M,D=D,B1=B1,B2=B2)
set.seed(1887)
fbi <- ulam(
    alist(
        # mom model
            M ~ normal( mu , sigma ),
            mu <- a1 + b*B1 + k*U[i],
        # daughter model
            D ~ normal( nu , tau ),
            nu <- a2 + b*B2 + m*M + k*U[i],
        # B1 and B2
            B1 ~ bernoulli(p),
            B2 ~ bernoulli(p),
        # unmeasured confound
            vector[N]:U ~ normal(0,1),
        # priors
            c(a1,a2,b,m) ~ normal( 0 , 0.5 ),
            c(k,sigma,tau) ~ exponential( 1 ),
            p ~ beta(2,2)
    ), data=dat , chains=4 , cores=4 , iter=2000 , cmdstan=TRUE )

precis(fbi)
```

Comparing the results of Bayesian inference and linear regressions above.

```{r}
m <- M
plot( coeftab( lm( D ~ m ) , lm( D ~ m + B1 ) ,  lm( D ~ m + B2 ) , fbi ) , pars="m" )
```

A strength of a Bayesian inference model is that it is a generative model, which means that we can extract or draw samples from the fit models. We can compare simulated with estimated confounding factors.

```{r}
post <- extract.samples(fbi)
Uest <- apply(post$U,2,mean) 
{
  blank()
  plot(U, Uest, xlab="U (simulated)",ylab="U (estimated)", col=2 , lwd=2 )
  abline(a=0,b=1,lty=2)
}
```

Alternative to setting up an explicit term for the unobserved confounding factor, we can also assume that mom and daughter are multi-normally distributed.

```{r}
# version that marginalizes out the missing data
fbi_plus <- ulam(
    alist(
        c(M,D) ~ multi_normal( c(mu,nu) , Rho , Sigma ),
        mu <- a1 + b*B1,
        nu <- a2 + b*B2 + m*M,
        c(a1,a2,b,m) ~ normal( 0 , 0.5 ),
        Rho ~ lkj_corr( 2 ),
        Sigma ~ exponential( 1 )
    ), data=dat , chains=4 , cores=4 , cmdstan=TRUE )

precis(fbi_plus,3)
```

Below is a more exotic example, where there is no instrument variable (B1 -> D), but two measures of U (V and W)
```{r}
set.seed(1887)
N <- 200 # number of pairs
U <- rnorm(N,0,1) # simulate confound
V <- rnorm(N,U,1)
W <- rnorm(N,U,1)
# birth order and family sizes
B1 <- rbinom(N,size=1,prob=0.5) # 50% first borns
M <- rnorm( N , 2*B1 + U )
B2 <- rbinom(N,size=1,prob=0.5)
D <- rnorm( N , 2*B2 + 0.5*B1 + U + 0*M )

# confounded regression
precis( lm( D ~ M + B1 + B2 + V + W ) )

# full-luxury bayesian inference
dat2 <- list(N=N,M=M,D=D,B1=B1,B2=B2,V=V,W=W)
fbi2 <- ulam(
    alist(
        M ~ normal( muM , sigmaM ),
        muM <- a1 + b*B1 + k*U[i],
        D ~ normal( muD , sigmaD ),
        muD <- a2 + b*B2 + d*B1 + m*M + k*U[i],
        W ~ normal( muW , sigmaW ),
        muW <- a3 + w*U[i],
        V ~ normal( muV , sigmaV ),
        muV <- a4 + v*U[i],
        vector[N]:U ~ normal(0,1),
        c(a1,a2,a3,a4,b,d,m) ~ normal( 0 , 0.5 ),
        c(k,w,v) ~ exponential( 1 ),
        c(sigmaM,sigmaD,sigmaW,sigmaV) ~ exponential( 1 )
    ), data=dat2 , chains=4 , cores=4 , iter=2000 , cmdstan=TRUE )

precis(fbi2)
```

# The peer bias example

```{r noDisc}
library(rethinking)

# simulation in which there is no discrimination
# conditioning on E reveals the truth
set.seed(1887)
N <- 500
X <- rbern(N,prob=0.5)
pY <- c( 0.25 , 0.05 )
pE <- X*inv_logit(-2) + (1-X)*inv_logit(+1)
E <- sapply( 1:N , function(n) sample( 1:2 , size=1 , prob=c(pE[n],1-pE[n]) ) )

p <- pY[E]
Y <- rbern(N,prob=p)

precis( glm( Y ~ X , family=binomial ) )
```

```{r}
precis( glm( Y ~ X + E , family=binomial ) )
```


```{r peer, eval=FALSE}



mg0 <- glm( Y ~ X , family=binomial )
mg1 <- glm( Y ~ X + E , family=binomial )
plot( coeftab( mg0 , mg1 ) , pars="X" )


# simulation in which there really is discrimination
# condition on E hides the truth
set.seed(1914)
set.seed(1964)
N <- 500
Q <- rnorm(N)
X <- rbern(N,prob=0.5)
pY <- c( 0.25 , 0.1 )
pE <- X*inv_logit(Q-2) + (1-X)*inv_logit(Q+1)
E <- sapply( 1:N , function(n) sample( 1:2 , size=1 , prob=c(pE[n],1-pE[n]) ) )

p <- inv_logit( logit(pY[E]) + Q - X )
Y <- rbern(N,prob=p)

precis( glm( Y ~ X , family=binomial ) )

precis( glm( Y ~ X + E , family=binomial ) )

precis( glm( Y ~ X + E + Q , family=binomial ) )

mg0 <- glm( Y ~ X , family=binomial )
mg1 <- glm( Y ~ X + E , family=binomial )
mg2 <- glm( Y ~ X + E + Q , family=binomial )
plot( coeftab( mg0 , mg1 ) , pars="X" )

# descendants of Q
R1 <- rnorm(N,0.5*Q)
R2 <- rnorm(N,0.5*Q)

mg3 <- glm( Y ~ X + E + R1 + R2 , family=binomial )
precis( mg3 )
plot( coeftab( mg0 , mg1 , mg3 ) , pars="X" )

# bayes model
dat <- list( Y=Y , E=E , XX=X , id=1:N , R1=R1 , R2=R2 )
mR <- ulam(
    alist(
        # Y model
        Y ~ bernoulli(p),
        logit(p) <- a[E] + X*XX + h*Q[id],
        a[E] ~ normal(0,1),
        X ~ normal(0,1),
        h ~ half_normal(0,1),
        # Q model
        vector[id]:Q ~ normal(0,1),
        R1 ~ normal(Q,1),
        R2 ~ normal(Q,1)
    ) , data=dat , chains=4 , cores=4 , cmdstan=TRUE )

precis(mR,2,omit="Q")

plot( coeftab( mg0 , mg1 , mg3 , mR ) , pars="X" )

post <- extract.samples(mR)
Qest <- apply(post$Q,2,mean)
blank()
plot(Q,Qest)
abline(a=0,b=1,lty=2)

# confounded but we do a partial identification analysis
# we use an informative prior for h (effect of Q)

dat2 <- list( Y=Y , E=E , I=I , id=1:N )

m2 <- ulam(
    alist(
        # Y model
        Y ~ bernoulli(p),
        logit(p) <- a[E] + g*X + h*Q[id],
        a[E] ~ normal(0,1),
        g ~ normal(0,1),
        h ~ uniform(0,2),
        # Q model
        vector[id]:Q ~ normal(0,1)
    ) , data=dat2 , chains=4 , cores=4 )

precis(m2,2,omit="Q")

post <- extract.samples(m2)

plot( post$h , post$g , pch=16 , col=grau(0.2) , cex=2 , ylab="effect of I" , xlab="effect of Q" )
abline(h=0,lty=2)

quantile(post$h)

############################
# d-separation plots
a <- 0.7
cols <- c( col.alpha(1,a) , col.alpha(2,a) )

# pipe

N <- 1000
X <- rnorm(N)
Z <- rbern(N,inv_logit(X))
Y <- rnorm(N,(2*Z-1))

plot( X , Y , col=cols[Z+1] , pch=16 )
abline(lm(Y[Z==1]~X[Z==1]),col=2,lwd=3)
abline(lm(Y[Z==0]~X[Z==0]),col=1,lwd=3)
abline(lm(Y~X),lwd=3,lty=3)

# fork

N <- 1000
Z <- rbern(N)
X <- rnorm(N,2*Z-1)
Y <- rnorm(N,(2*Z-1))

plot( X , Y , col=cols[Z+1] , pch=16 )
abline(lm(Y[Z==1]~X[Z==1]),col=2,lwd=3)
abline(lm(Y[Z==0]~X[Z==0]),col=1,lwd=3)
abline(lm(Y~X),lwd=3,lty=3)

# collider

N <- 1000
X <- rnorm(N)
Y <- rnorm(N)
Z <- rbern(N,inv_logit(2*X+2*Y-2))

plot( X , Y , col=cols[Z+1] , pch=16 )
abline(lm(Y[Z==1]~X[Z==1]),col=2,lwd=3)
abline(lm(Y[Z==0]~X[Z==0]),col=1,lwd=3)
abline(lm(Y~X),lwd=3,lty=3)
```

